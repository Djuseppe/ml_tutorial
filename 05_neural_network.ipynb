{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nbdev'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-983cbbe9abda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnbdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nbdev'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "> Summary: Classification, Neural Network, Gradient Descent, Backpropagation, Overfitting, Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a neural network?\n",
    "\n",
    "Without delving into brain analogies, I simply describe neural network as an algorithm for approximating functions. In the context of machine learning, neural network is a function that maps input to desired output, given a set of inputs $x$ and output $y$.\n",
    "\n",
    "Neural networks are a collection of a densely interconnected set of simple units, organazied into a *input layer*, one or more *hidden layers* and an *output layer*.\n",
    "\n",
    "The diagram below shows an architecture of a 3-layer neural network.\n",
    "\n",
    "![](images/nn_ex_2.jpeg)\n",
    "*Fig1. A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. [[Image Source]](http://cs231n.github.io/neural-networks-1/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Perceptron is a single neuron that calculates a linear combination of the input (i.e. performs a dot product with the input and its weights), adds a bias $b$, applies the non-linearity (or activation function), and outputs a value. If the activation function is sigmoid $\\sigma$, the output is a number between 0 and 1.\n",
    "\n",
    "$$\\hat{y}=\\sigma(b + \\sum_{i=1}^n w_ix_i)$$\n",
    "\n",
    "where $\\sigma (t)=\\frac{1}{1+e^{-t}}$\n",
    "\n",
    "![](images/nn_perceptron.jpeg)\n",
    "*Fig2. Representation of Perceptron*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Activation Functions\n",
    "\n",
    "1- **Sigmoid**. Sigmoid function takes a real-valued number and \"squashes\" it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1.\n",
    "\n",
    "2- **Tanh**. Tanh function $tanh(x)=2\\sigma(2x)-1$ squashes a real-valued number to the range [-1, 1]. Its output is zero-centered. Please note that the tanh neuron is simply a scaled sigmoid neuron.\n",
    "\n",
    "3- **ReLU**. The Rectified Linear Unit has become very popular in the last few years. It computes the function $f(x)=max(0,x)$. In other words, the activation is simply thresholded at zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Activation Functions\n",
    "\n",
    "The purpose of activation functions is to introduce ***non-linearity*** into the network, which enables the network to separate the data that is not linearly separable. Linear functions always create linear decisions while non-linear functions allow to approximate arbitrarily complex functions.\n",
    "\n",
    "![](images/nn_linear.png)\n",
    "![](images/nn_nonlinear.png)\n",
    "*Fig3. Left: Linear activation functions produce linear decisions. Right: Non-Linear activation functions produce non-linear decisions*. You can play with these examples in this [ConvNetsJS demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Simplified\n",
    "\n",
    "We usually simplify the perceptron representation as below.\n",
    "\n",
    "![](images/nn_perceptron_2.jpeg)\n",
    "*Fig5. Simplified perceptron*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network\n",
    "\n",
    "We will see how we can train a neural network through an example. Let's assume that our neural network architecture looks like the image shown below.\n",
    "\n",
    "![](images/nn_ex_1.jpeg)\n",
    "\n",
    "We can see that the weights $\\mathbf{W}$ and biases $\\mathbf{b}$ are the only variables that affect the output $\\hat{y}$. Therefore, training a neural network essentially means finding the right values for the weights and biases so that they can determine the best predictions. In other words, the process of fine-tuning the weights and biases from the input data is called training neural network.\n",
    "\n",
    "Training a neural network involves two steps:\n",
    "\n",
    "- Feed-forward computation\n",
    "   \n",
    "- backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward computation\n",
    "\n",
    "Feed-forward step fundamentally means *repeated matrix multiplications combined with activation function*. Considering our example network, we convert the different variables into vectors and matrices. Therefore, the input  would be a [3x1] vector. The weights of the hidden layer `W1` would be a [4x3] matrix, and its biases `b1` would be a [4x1] vector. In this layer, each neuron has its weights in a row of `W1`, so the matrix vector multiplication `np.dot(W1,x)` evaluates the activations of all neurons in this layer. For the last layer (output layer), `W2` and `b2` would be of size [1x4] and [1x1], respectively. The full forward pass of this 3-layer neural network is then simply three matrix multiplications, merged with the application of the activation function:\n",
    "\n",
    "![](images/nn_forward.jpeg)\n",
    "\n",
    "Therefore, the forward-pass is:\n",
    "\n",
    "![](images/nn_forward_full.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to learn the Parameters `W1,W2,b1,b2` of the above network. Please note that the input `x` could be an entire batch of training data, where each example would be a column of `x`.\n",
    "\n",
    "> Note: The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "performing a forward-pass of the network gives us the predictions. Therefore, we must evaluate the \"goodness\" of our predictions, which means we need to measure how far off our predictions are. **Loss function** enables us to do that. The loss function measures the cost caused by incorrect predictions.\n",
    "\n",
    "if $\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})$ is the loss of each example $i$ in the training set, for $i=1,2,\\cdots,m$, then total loss $J(W)$ over the entire dataset is:\n",
    "\n",
    "$$J(W)=\\frac{1}{m}\\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)},y^{(i)})$$\n",
    "\n",
    "Please note that $W=[W^{(1)}, W^{(2)},\\cdots, W^{(L)}]$, where $W^{(j)}$ is matrix of weights of layer $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "Cross entropy loss is used in classification problems and outputs a probability between 0 and 0.\n",
    "\n",
    "$$J(W)=\\frac{1}{m}\\sum_{i=1}^m \\left[ y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error Loss\n",
    "\n",
    "Mean squared error loss is used in regression problems and outputs a real-valued number.\n",
    "\n",
    "$$J(W)=\\frac{1}{m}\\sum_{i=1}^m \\left( y^{(i)} - \\hat{y}^{(i)} \\right)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "\n",
    "Our goal in training is to find the weights and biases that minimizes the loss function:\n",
    "\n",
    "$$W^* = \\underset{W}{\\arg\\min}\\ J(W)$$\n",
    "\n",
    "After we measured the loss, we need to find a way to **propagate** the error back, and update our weights and biases, in order to decrease the loss. But how much should we adjust the weights and biases.\n",
    "\n",
    "In order to find the appropriate amount of change in the weights and biases, we need to take the derivative of the loss function with respect to the weights and biases. The process of computing gradients of expressions through recursive application of chain rule is called **backpropagation**. When we have the gradients, we can use the *gradient descent* algorithm to update the weights.\n",
    "\n",
    "Gradient descent algorithm:\n",
    "1. Initialize the weights randomly $\\sim N(0,\\sigma^2)$\n",
    "2. Loop until convergence:\n",
    "   1. Compute gradient, $\\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}}$\n",
    "   2. Update weights, $W = W-\\eta \\frac{\\partial J(W)}{\\partial W}$\n",
    "3. Return weights\n",
    "\n",
    "For instance, if we are going to compute the gradient with respect to $w_1$, in our example network below, we have:\n",
    "![](images/nn_backprop.jpeg)\n",
    "\n",
    "We need to repeat this **for every weight in the network** using gradients from later layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full backpropagation of our model is: ($\\odot$ is the element-wise multiplication)\n",
    "\n",
    "![](images/nn_backprop_full.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "x = np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]))\n",
    "y = np.array(([0],[1],[1],[0]))\n",
    "\n",
    "n_hidden_units = 10\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.y_hat = np. zeros(y.shape)\n",
    "        \n",
    "        self.W1 = np.random.randn(x.shape[1],n_hidden_units)\n",
    "        self.W2 = np.random.randn(n_hidden_units,1)\n",
    "        self.b1 = np.zeros((1,n_hidden_units))\n",
    "        self.b2 = np.zeros((1,1))\n",
    "        self.learning_rate = 0.01\n",
    "        \n",
    "          \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def deriv_sigmoid(self,x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    \n",
    "    def feedforward(self):\n",
    "        z1 = np.dot(self.x,self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(z1)\n",
    "        z2 = np.dot(self.a1,self.W2) + self.b2\n",
    "        self.y_hat  = self.sigmoid(z2)\n",
    "\n",
    "\n",
    "    def cross_entropy(self):\n",
    "        loss = -np.mean(self.y * np.log(self.y_hat) + (1 - self.y) * np.log(1 - self.y_hat))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def backprop(self):\n",
    "        delta_2 = self.y_hat - self.y\n",
    "        dW2 = np.dot(self.a1.T,delta_2)\n",
    "        db2 = delta_2\n",
    "        z2_delta = np.dot(delta_2,self.W2.T)\n",
    "        delta_1 = z2_delta * self.deriv_sigmoid(self.a1)\n",
    "        dW1 = np.dot(self.x.T,delta_1)\n",
    "        db1 = delta_1\n",
    "    \n",
    "        self.W2 = self.W2 - self.learning_rate * dW2\n",
    "        self.b2 = self.b2 - self.learning_rate * db2\n",
    "        self.W1 = self.W1 - self.learning_rate * dW1\n",
    "        self.b1 = self.b1 - self.learning_rate * db1\n",
    "\n",
    "#         self.W2 -= self.learning_rate * dW2\n",
    "#         self.b2 -= self.learning_rate * db2\n",
    "#         self.W1 -= self.learning_rate * dW1\n",
    "#         self.b1 -= self.learning_rate * db1\n",
    "        \n",
    "    def train(self):\n",
    "        self.feedforward()\n",
    "        self.backprop()\n",
    "\n",
    "\n",
    "    \n",
    "nn = NeuralNetwork(x,y)\n",
    "\n",
    "for i in range(1,1001):\n",
    "    nn.train()\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iteration: {}\".format(i))\n",
    "        print(\"Actual Output: {}\".format(nn.y.flatten()))\n",
    "        print(\"Predicted Output: {}\".format(nn.y_hat.flatten()))\n",
    "        loss = nn.cross_entropy()\n",
    "        print(\"loss: {}\".format(loss))    \n",
    "\n",
    "bb1 = nn.b2\n",
    "bb1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration # 1000\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01382207]\n",
      " [0.96366289]\n",
      " [0.96485941]\n",
      " [0.04136934]]\n",
      "Loss: \n",
      "0.0011144296435900503\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import numpy as np \n",
    "      \n",
    "# Each row is a training example, each column is a feature  [X1, X2, X3]\n",
    "X=np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "y=np.array(([0],[1],[1],[0]), dtype=float)\n",
    "\n",
    "# Define useful functions    \n",
    "\n",
    "# Activation function\n",
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(p):\n",
    "    return p * (1 - p)\n",
    "\n",
    "# Class definition\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x,y):\n",
    "        self.input = x\n",
    "        self.weights1= np.random.rand(self.input.shape[1],4) # considering we have 4 nodes in the hidden layer\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y = y\n",
    "        self.output = np. zeros(y.shape)\n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "#         print(\"layer1:\",self.layer1.shape)\n",
    "        self.layer2 = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "#         print(\"layer2:\",self.layer1.shape)\n",
    "        return self.layer2\n",
    "        \n",
    "    def backprop(self):\n",
    "        d_weights2 = np.dot(self.layer1.T, 2*(self.y -self.output)*sigmoid_derivative(self.output))\n",
    "        d_weights1 = np.dot(self.input.T, np.dot(2*(self.y -self.output)*sigmoid_derivative(self.output), self.weights2.T)*sigmoid_derivative(self.layer1))\n",
    "    \n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.output = self.feedforward()\n",
    "        self.backprop()\n",
    "        \n",
    "\n",
    "NN = NeuralNetwork(X,y)\n",
    "for i in range(1,1001): # trains the NN 1,000 times\n",
    "    if i % 1000 ==0: \n",
    "        print (\"for iteration # \" + str(i) + \"\\n\")\n",
    "        print (\"Input : \\n\" + str(X))\n",
    "        print (\"Actual Output: \\n\" + str(y))\n",
    "        print (\"Predicted Output: \\n\" + str(NN.feedforward()))\n",
    "        print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.feedforward())))) # mean sum squared loss\n",
    "        print (\"\\n\")\n",
    "  \n",
    "    NN.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
