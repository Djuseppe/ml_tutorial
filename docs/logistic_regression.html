---

title: Logistic Regression

keywords: fastai
sidebar: home_sidebar

summary: "Summary: Classification, Logistic Regression, Gradient Descent, Overfitting, Regularization"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 02_logistic_regression.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Definition">Definition<a class="anchor-link" href="#Definition">&#182;</a></h2><p>Logistic regression is a classification technique used for binary classification problems such as classifying tumors as malignant / not malignant, classifying emails as spam / not spam.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Recap">Recap<a class="anchor-link" href="#Recap">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Classification">Classification<a class="anchor-link" href="#Classification">&#182;</a></h3><p><strong>Classification</strong> is a learning algorithm that determines which discrete category a new example (instance) belongs, given a set of training instances $X$ with their observed categories $Y$. Binary classification is a classification task where $Y$ has two possible values $0,1$. If $Y$ has more than two possible values, it is called a multi-class classification.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Can-we-use-Linear-Regression-for-classification-problems?">Can we use Linear Regression for classification problems?<a class="anchor-link" href="#Can-we-use-Linear-Regression-for-classification-problems?">&#182;</a></h3><p>Thare are two main issues with using linear regression for classification:</p>
<ol>
<li><p>Linear regression outputs values for $Y$ that can be much larger than 1 or much lower than 0, but our classes are 0 and 1.</p>
</li>
<li><p>Our hypothesis or prediction rule can change each time a new training example arrives, which shouldn't. Instead, we should be able to use the learned hypothesis to make correct predictions for the data we haven't seen before.</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Naive-Bayes">Naive Bayes<a class="anchor-link" href="#Naive-Bayes">&#182;</a></h3><p>If we have consider Naive Bayes assumption, then we have:
$$P(X_1\cdots X_d|Y)=\prod_{i=1}^{d}P(X_i|Y)$$</p>
<p>We also assume parametric form for $P(X_i|Y)$ and $P(Y)$. Then, we use MLE or MAP to estimate the parameters.</p>
<p>At last, Naive Bayes classifier for a $X^{new} = &lt;X_1,X_2,\cdots X_d&gt;$ is:</p>
$$Y^{new} =\underset{y}{\arg\max}\quad P(Y=y_k)\prod_i P(X^{new}_i|Y=y_K)$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generative-vs-Discriminative-Classifiers">Generative vs Discriminative Classifiers<a class="anchor-link" href="#Generative-vs-Discriminative-Classifiers">&#182;</a></h2><p>Training a classifier involves estimating $P(Y|X)$.</p>
<p>i. Generative classifiers (e.g. Naive Bayes):</p>
<ul>
<li>Assume some functional form for $P(X,Y)$, i.e. $P(X|Y)$ and $P(Y)$</li>
<li>Estimate parameters of $P(X|Y)$, $P(Y)$ directly from training data</li>
<li>$\underset{y}{\arg\max}\quad P(X|Y)P(Y)= \underset{y}{\arg\max}\quad P(Y|X)$</li>
</ul>
<p><strong>Question:</strong> <em>Can we learn the $P(Y|X)$ directly from data? Or better yet, can we learn decision boundary directly?</em></p>
<p>ii. Discriminative classifiers (e.g. Logistic Regression):</p>
<ul>
<li>Assume some functional form for $P(Y|X)$ or for the decision boundary</li>
<li>Estimate parameters of $P(Y|X)$ directly from training data</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Logistic-Regression">Logistic Regression<a class="anchor-link" href="#Logistic-Regression">&#182;</a></h2><p>Logistic regression is a classification method for binary classification problems, where input $X$ is a vector of discrete or real-valued variables and $Y$ is discrete (boolean valued). The idea is to learn $P(Y|X)$ directly from observed data.</p>
<p>Let's consider learning $f:X\rightarrow Y$ where,</p>
<ul>
<li>$X$ is a vector of real-valued features, $&lt;X_1,\cdots,X_n&gt;$</li>
<li>$Y$ is boolean</li>
<li>Assume all $X_i$ are conditionally independent given $Y$</li>
<li>Assume $P(X_i|Y=y_k) \sim N(\mu_{ik},\sigma_i)$</li>
<li>Assume $P(Y) \sim $ Bernoulli($\pi$)</li>
</ul>
<p>What does this imply about the form of $P(Y|X)$?</p>
$$P(Y=0|X=&lt;X_1,\cdots,X_n&gt;)=\frac{1}{1+exp(w_0+\sum_{i}w_iX_i)}$$$$P(Y=1|X) = 1 - P(Y=1|X) = \frac{\exp(w_0+\sum_{i=1}^{n}w_iX_i)}{1+\exp(w_0+\sum_{i=1}^{n}w_iX_i)} $$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Logistic-Function">Logistic Function<a class="anchor-link" href="#Logistic-Function">&#182;</a></h2><p>In Logistic regression $P(Y|X)$ follows the form of the sigmoid function, which means logistic regression gives the <em>probability</em> that an instance belongs to class $1$ or class $0$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="sigmoid" class="doc_header"><code>sigmoid</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/logistic_regression.py#L6" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>sigmoid</code>(<strong><code>z</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/ml_tutorial/images/sigmoid.svg" alt="sigmoid">
<em>Sigmoid Function</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Logistic-Regression-is-Linear">Logistic Regression is Linear<a class="anchor-link" href="#Logistic-Regression-is-Linear">&#182;</a></h2><p>The reason that logistic regression is linear is that, the outcome is a linear combinations of the inputs and parameters.</p>
$$\frac{P(Y=1|X)}{P(Y=0|X)} = \exp(w_0+\sum_{i=1}^{n}w_iX_i)$$<p>which implies:</p>
$$\ln\frac{P(Y=1|X)}{P(Y=0|X)} = w_0+\sum_{i=1}^{n}w_iX_i$$<p>where $w_0+\sum_{i=1}^{n}w_iX_i$ is the linear classification rule.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-Logistic-Regression:-MLCE">Training Logistic Regression: MLCE<a class="anchor-link" href="#Training-Logistic-Regression:-MLCE">&#182;</a></h2><p>We have a collection of training data $D = \{&lt;X^1,Y^1&gt;,\cdots,&lt;X^M,Y^M&gt;\}$. We need to find the parameters $\mathbf{w}=&lt;w_0,\cdots,w_n&gt;$ that <strong>maximize the conditional likelihood</strong> of the training data.</p>
<p>Data likelihood $=\prod_j P(&lt;X^j,Y^j&gt;|\mathbf{w})$, thus data <strong>conditional</strong> likelihood $=\prod_j P(Y^j|X^j,\mathbf{w})$.</p>
<p>therefore,
$$W_{MLCE}=\underset{\mathbf{w}}{\arg\max}\prod_j P(Y^j|X^j,\mathbf{w})$$</p>
<p>In order to make arithmetic easier, we work with the conditional log likelihood. Additionally, we know that maximizing a function is equivalent to <em>minimizing the negative of the function</em>. Therefore, we convert our problem to a minimization problem and apply the Gradient Descent algorithm to find the minimum.
$$
\begin{aligned}
  W_{MLCE}&amp;=\underset{\mathbf{w}}{\arg\max}\quad \ln \prod_j P(Y^j|X^j,\mathbf{w}) =  \sum_j \ln P(Y^j|X^j,\mathbf{w})\\
  &amp;=\underset{W}{\arg\max}\quad\sum_j \ln P(Y^j|X^j,\mathbf{w})\\
  &amp;=\underset{W}{\arg\min}\quad -\sum_j \ln P(Y^j|X^j,\mathbf{w})
\end{aligned}
$$</p>
<p>if $J(\mathbf{w})=-\sum_j \ln P(Y^j|X^j,\mathbf{w})$, then we have:</p>
$$\begin{aligned}
  J(\mathbf{w})&amp;=-\sum_j \left [ Y^j \ln P(Y^j=1|X^j,\mathbf{w}) + (1-Y^j) \ln P(Y^j=0|X^j,\mathbf{w})\right ]\\
  &amp;=-\sum_j \left[ Y^j(w_0+\sum_iw_iX_i^j)-\ln(1+\exp(w_0+\sum_iw_iX_i^j))\right ]
\end{aligned}
$$<p>$J(\mathbf{w})$ is a convex function, so we can always find global minimum. There is no closed-form solution to minimize it. However, we can use gradient descent algorithm to find the minimum.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-Descent-algorithm-for-Logistic-Regression">Gradient Descent algorithm for Logistic Regression<a class="anchor-link" href="#Gradient-Descent-algorithm-for-Logistic-Regression">&#182;</a></h2><ol>
<li>Compute the gradient of $J_D(\mathbf{w})$ over the entire training set $D$:
$$\nabla J_D(\mathbf{w}) = \left [\frac{\partial J_D(\mathbf{w})}{\partial w_0},\cdots \frac{\partial J_D(\mathbf{w})}{\partial w_n} \right]$$</li>
</ol>
$$\frac{\partial J_D(\mathbf{w})}{\partial w_i}=\sum_{j=1}^{M}X_i^j\left[Y^j-\hat{P}(Y^j=1|X^j,\mathbf{w})\right]$$<p>We assume $X_0^j=1$, (for $j=0,1,\cdots M$)</p>
<ol>
<li>Do until satisfied:<ul>
<li>Update the vector of parameters: $w_i=w_i-\eta \frac{\partial J_D(\mathbf{w})}{\partial w_i}$, (for $i=0,1,\cdots n$)</li>
</ul>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-Maximum-a-Posteriori-(MAP)-to-Estimate-Parameters">Using Maximum a Posteriori (MAP) to Estimate Parameters<a class="anchor-link" href="#Using-Maximum-a-Posteriori-(MAP)-to-Estimate-Parameters">&#182;</a></h2><p>We assume Gaussian distributions for the prior: $\mathbf{w} \sim N(0,\sigma I)$.Thus,</p>
$$\mathbf{w^*}=\underset{\mathbf{w}}{\arg\max}\quad \ln\left[P(\mathbf{w})\prod_j P(Y^j|X^j,\mathbf{w})\right]$$<p>Therefore,</p>
$$J(\mathbf{w})=-\sum_j \left[ Y^j \ln P(Y^j=1|X^j,\mathbf{w}) + (1-Y^j) \ln P(Y^j=0|X^j,\mathbf{w})\right]+\lambda\sum_{i=1}^{n}w_i^2$$<p>where $\lambda \sum_{i=1}^{n}w_i^2$ is called a <strong>regularization</strong> term. Regularization helps reduce the overfitting, and also keeps the weights near to zero (i.e. discourage the weights from getting large values).</p>
<p>And the modified gradient descent rule is:</p>
$$w_i=w_i-\eta \left [\frac{\partial J_D(\mathbf{w})}{\partial w_i} + \lambda w_i \right ]$$
</div>
</div>
</div>
</div>
 

