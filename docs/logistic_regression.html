---

title: Logistic Regression

keywords: fastai
sidebar: home_sidebar

summary: "Summary: Classification, Logistic Regression, Gradient Descent, Overfitting, Regularization"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 02_logistic_regression.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Definition">Definition<a class="anchor-link" href="#Definition">&#182;</a></h2><p>Logistic regression is a classification technique used for binary classification problems such as classifying tumors as malignant / not malignant, classifying emails as spam / not spam.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Recap">Recap<a class="anchor-link" href="#Recap">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Classification">Classification<a class="anchor-link" href="#Classification">&#182;</a></h3><p><strong>Classification</strong> is a learning algorithm that determines which discrete category a new example (instance) belongs, given a set of training instances $X$ with their observed categories $Y$. Binary classification is a classification task where $Y$ has two possible values $0,1$. If $Y$ has more than two possible values, it is called a multi-class classification.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Can-we-use-Linear-Regression-for-classification-problems?">Can we use Linear Regression for classification problems?<a class="anchor-link" href="#Can-we-use-Linear-Regression-for-classification-problems?">&#182;</a></h3><p>Thare are two main issues with using linear regression for classification:</p>
<ol>
<li><p>Linear regression outputs values for $Y$ that can be much larger than 1 or much lower than 0, but our classes are 0 and 1.</p>
</li>
<li><p>Our hypothesis or prediction rule can change each time a new training example arrives, which shouldn't. Instead, we should be able to use the learned hypothesis to make correct predictions for the data we haven't seen before.</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Naive-Bayes">Naive Bayes<a class="anchor-link" href="#Naive-Bayes">&#182;</a></h3><p>If we have consider Naive Bayes assumption, then we have:
$$P(X_1\cdots X_d|Y)=\prod_{i=1}^{d}P(X_i|Y)$$</p>
<p>We also assume parametric form for $P(X_i|Y)$ and $P(Y)$. Then, we use MLE or MAP to estimate the parameters.</p>
<p>At last, Naive Bayes classifier for a $X^{new} = &lt;X_1,X_2,\cdots X_d&gt;$ is:</p>
$$Y^{new} =\underset{y}{\arg\max}\quad P(Y=y_k)\prod_i P(X^{new}_i|Y=y_K)$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generative-vs-Discriminative-Classifiers">Generative vs Discriminative Classifiers<a class="anchor-link" href="#Generative-vs-Discriminative-Classifiers">&#182;</a></h2><p>Training a classifier involves estimating $P(Y|X)$.</p>
<p>i. Generative classifiers (e.g. Naive Bayes):</p>
<ul>
<li>Assume some functional form for $P(X,Y)$, i.e. $P(X|Y)$ and $P(Y)$</li>
<li>Estimate parameters of $P(X|Y)$, $P(Y)$ directly from training data</li>
<li>$\underset{y}{\arg\max}\quad P(X|Y)P(Y)= \underset{y}{\arg\max}\quad P(Y|X)$</li>
</ul>
<p><strong>Question:</strong> <em>Can we learn the $P(Y|X)$ directly from data? Or better yet, can we learn decision boundary directly?</em></p>
<p>ii. Discriminative classifiers (e.g. Logistic Regression):</p>
<ul>
<li>Assume some functional form for $P(Y|X)$ or for the decision boundary</li>
<li>Estimate parameters of $P(Y|X)$ directly from training data</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Logistic-Regression">Logistic Regression<a class="anchor-link" href="#Logistic-Regression">&#182;</a></h2><p>Logistic regression is a classification method for binary classification problems, where input $X$ is a vector of discrete or real-valued variables and $Y$ is discrete (boolean valued). The idea is to learn $P(Y|X)$ directly from observed data.</p>
<p>Let's consider learning $f:X\rightarrow Y$ where,</p>
<ul>
<li>$X$ is a vector of real-valued features, $&lt;X_1,\cdots,X_n&gt;$</li>
<li>$Y$ is boolean</li>
<li>Assume all $X_i$ are conditionally independent given $Y$</li>
<li>Assume $P(X_i|Y=y_k) \sim N(\mu_{ik},\sigma_i)$</li>
<li>Assume $P(Y) \sim $ Bernoulli($\pi$)</li>
</ul>
<p>What does this imply about the form of $P(Y|X)$?</p>
$$P(Y=0|X=&lt;X_1,\cdots,X_n&gt;)=\frac{1}{1+exp(w_0+\sum_{i}w_iX_i)}$$$$P(Y=1|X) = 1 - P(Y=1|X) = \frac{\exp(w_0+\sum_{i=1}^{n}w_iX_i)}{1+\exp(w_0+\sum_{i=1}^{n}w_iX_i)} $$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Logistic-Function">Logistic Function<a class="anchor-link" href="#Logistic-Function">&#182;</a></h2><p>In Logistic regression $P(Y|X)$ follows the form of the sigmoid function, which means logistic regression gives the <em>probability</em> that an instance belongs to class $1$ or class $0$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="sigmoid" class="doc_header"><code>sigmoid</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/logistic_regression.py#L6" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>sigmoid</code>(<strong><code>z</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/ml_tutorial/images/sigmoid.svg" alt="sigmoid">
<em>Fig1. Sigmoid Function</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Logistic-Regression-is-Linear">Logistic Regression is Linear<a class="anchor-link" href="#Logistic-Regression-is-Linear">&#182;</a></h2><p>The reason that logistic regression is linear is that, the outcome is a linear combinations of the inputs and parameters.</p>
$$\frac{P(Y=1|X)}{P(Y=0|X)} = \exp(w_0+\sum_{i=1}^{n}w_iX_i)$$<p>which implies:</p>
$$\ln\frac{P(Y=1|X)}{P(Y=0|X)} = w_0+\sum_{i=1}^{n}w_iX_i$$<p>where $w_0+\sum_{i=1}^{n}w_iX_i$ is the linear classification rule.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Decision-Boundary">Decision Boundary<a class="anchor-link" href="#Decision-Boundary">&#182;</a></h2><p>Since logistic regression prediction function returns a probability between 0 and 1, in order to predict which class this data belongs we need to set a threshold. For each data point, if the estimated probability is above this threshold, we classify the point into class 1, and if it's below the threshold, we classify the point into class 2.</p>
<p>If $P(Y=1|X)\geq0.5$, class $=1$, and if $P(Y=1|X)&lt;0.5$, class $=0$.
{% include note.html content='Decision boundary can be linear (a line) or non-linear (a curve or higher order polynomial). Polynomial order can be increased to get complex decision boundary.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-Logistic-Regression:-MLCE">Training Logistic Regression: MLCE<a class="anchor-link" href="#Training-Logistic-Regression:-MLCE">&#182;</a></h2><p>We have a collection of training data $D = \{&lt;X^1,Y^1&gt;,\cdots,&lt;X^M,Y^M&gt;\}$. We need to find the parameters $\mathbf{w}=&lt;w_0,\cdots,w_n&gt;$ that <strong>maximize the conditional likelihood</strong> of the training data.</p>
<p>Data likelihood $=\prod_j P(&lt;X^j,Y^j&gt;|\mathbf{w})$, thus data <strong>conditional</strong> likelihood $=\prod_j P(Y^j|X^j,\mathbf{w})$.</p>
<p>therefore,
$$W_{MLCE}=\underset{\mathbf{w}}{\arg\max}\prod_j P(Y^j|X^j,\mathbf{w})$$</p>
<p>In order to make arithmetic easier, we work with the conditional log likelihood. Additionally, we know that maximizing a function is equivalent to <em>minimizing the negative of the function</em>. Therefore, we convert our problem to a minimization problem and apply the Gradient Descent algorithm to find the minimum.
$$
\begin{aligned}
  W_{MLCE}&amp;=\underset{\mathbf{w}}{\arg\max}\quad \ln \prod_j P(Y^j|X^j,\mathbf{w}) =  \sum_j \ln P(Y^j|X^j,\mathbf{w})\\
  &amp;=\underset{W}{\arg\max}\quad\sum_j \ln P(Y^j|X^j,\mathbf{w})\\
  &amp;=\underset{W}{\arg\min}\quad -\sum_j \ln P(Y^j|X^j,\mathbf{w})
\end{aligned}
$$</p>
<p>if $J(\mathbf{w})=-\sum_j \ln P(Y^j|X^j,\mathbf{w})$, then we have:</p>
$$\begin{aligned}
  J(\mathbf{w})&amp;=-\sum_j \left [ Y^j \ln P(Y^j=1|X^j,\mathbf{w}) + (1-Y^j) \ln P(Y^j=0|X^j,\mathbf{w})\right ]\\
  &amp;=-\sum_j \left[ Y^j(w_0+\sum_iw_iX_i^j)-\ln(1+\exp(w_0+\sum_iw_iX_i^j))\right ]
\end{aligned}
$$<p>$J(\mathbf{w})$ is a convex function, so we can always find global minimum. There is no closed-form solution to minimize it. However, we can use gradient descent algorithm to find the minimum.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-Descent-algorithm-for-Logistic-Regression">Gradient Descent algorithm for Logistic Regression<a class="anchor-link" href="#Gradient-Descent-algorithm-for-Logistic-Regression">&#182;</a></h2><ol>
<li>Compute the gradient of $J_D(\mathbf{w})$ over the entire training set $D$:
$$\nabla J_D(\mathbf{w}) = \left [\frac{\partial J_D(\mathbf{w})}{\partial w_0},\cdots \frac{\partial J_D(\mathbf{w})}{\partial w_n} \right]$$</li>
</ol>
$$\frac{\partial J_D(\mathbf{w})}{\partial w_i}=\sum_{j=1}^{M}X_i^j\left[Y^j-\hat{P}(Y^j=1|X^j,\mathbf{w})\right]$$<p>We assume $X_0^j=1$, (for $j=0,1,\cdots M$)</p>
<ol>
<li>Do until satisfied:<ul>
<li>Update the vector of parameters: $w_i=w_i-\eta \frac{\partial J_D(\mathbf{w})}{\partial w_i}$, (for $i=0,1,\cdots n$)</li>
</ul>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-Maximum-a-Posteriori-(MAP)-to-Estimate-Parameters">Using Maximum a Posteriori (MAP) to Estimate Parameters<a class="anchor-link" href="#Using-Maximum-a-Posteriori-(MAP)-to-Estimate-Parameters">&#182;</a></h2><p>We assume Gaussian distributions for the prior: $\mathbf{w} \sim N(0,\sigma I)$.Thus,</p>
$$\mathbf{w^*}=\underset{\mathbf{w}}{\arg\max}\quad \ln\left[P(\mathbf{w})\prod_j P(Y^j|X^j,\mathbf{w})\right]$$<p>Therefore,</p>
$$J(\mathbf{w})=-\sum_j \left[ Y^j \ln P(Y^j=1|X^j,\mathbf{w}) + (1-Y^j) \ln P(Y^j=0|X^j,\mathbf{w})\right]+\lambda\sum_{i=1}^{n}w_i^2$$<p>where $\lambda \sum_{i=1}^{n}w_i^2$ is called a <strong>regularization</strong> term. Regularization helps reduce the overfitting, and also keeps the weights near to zero (i.e. discourage the weights from getting large values).</p>
<p>And the modified gradient descent rule is:</p>
$$w_i=w_i-\eta \left [\frac{\partial J_D(\mathbf{w})}{\partial w_i} + \lambda w_i \right ]$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Logistic-Regression-Model-Fitting-using-Cost-Function">Logistic Regression Model Fitting using Cost Function<a class="anchor-link" href="#Logistic-Regression-Model-Fitting-using-Cost-Function">&#182;</a></h2><p>In the first part above, we learned how to fit parameters for logistic regression model using two primary principles: (a) maximum likelihood (conditional) estimates (MLE), and (b) map a posteriori estimation. In this section, we show how to train a logistic regression model (i.e. find the parameters $w$) by defining and minimizing a <strong>cost</strong> function.</p>
<p>In general, while training a classification model, our goal is to find a model that minimizes error. For logistic regression, we would like to find the parameters $w$ that minimize the number of misclassifications. Therefore, we define a cost function based on the parameters and find the set of parameters that give the minimum cost/error.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cost-Function-for-Logistic-Regression">Cost Function for Logistic Regression<a class="anchor-link" href="#Cost-Function-for-Logistic-Regression">&#182;</a></h2><p><em>A cost function primarily measures how wrong a machine learning model is in terms of estimating the relationship between $X$ and $y$.</em> Therefore, a cost function aims to penalize bad choices for the parameters to be optimized.</p>
<p>We can not use the linear regression cost function for logistic regression. If we make use of mean squared error for logistic regression, we will end up with a non-convex function of parameters $w$ (due to non-linear sigmoid function). Thus, gradient descent <em>cannot</em> guarantee that it finds the global minimum.</p>
<p><img src="/ml_tutorial/images/con_nonconvex.png" alt="convex">
<em>Fig2. Convex and non-convex functions. (<a href="&quot;https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc&quot;">image source</a>)</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Instead of Mean Squared Error, we use a cost function called <strong>Cross-entropy</strong> (or Log Loss). The cross-entropy cost functions can be divided into two functions: one for $y=0$ and one for $y=1$.</p>
<p>Training data:  $D = \{&lt;x^1,y^1&gt;,\cdots,&lt;x^m,y^m&gt;\}$.</p>
<p>Every instance has $n+1$ features. $x=&lt;x_0,\cdots,x_n&gt;$, where $x_0=1$</p>
<p>$w$ is the vector of parameters: $w=&lt;w_0,\cdots,w_n&gt;$</p>
<p>And, $h_{w}(x) = P(y=1|x,w)=\dfrac{1}{1+e^{-wx}}$</p>
<p>The cost functions for each data point $i$ are:</p>
<ul>
<li>$J_{y=1}^i(w)=-\log(h_{w}(x^i))$ if $y=1$</li>
<li>$J_{y=0}^i(w)=-\log(1-h_{w}(x^i))$ if $y=0$</li>
</ul>
<p>We can interpret cost functions as follows:</p>
<ol>
<li>if $y=1$:<ul>
<li>if $h_w(x)=0$ and $y=1$, cost is very large (infinite)</li>
<li>if $h_w(x)=1$ and $y=1$, cost $=0$</li>
</ul>
</li>
</ol>
<p><img src="/ml_tutorial/images/cost_1.svg" alt="&quot;cost Function&quot;">
<em>Fig3. cost function $-log(h(x))$ vs $h(x)$ when y=1</em></p>
<ol>
<li>if $y=0$:<ul>
<li>if $h_w(x)=1$ and $y=0$, cost is very large (infinite)<ul>
<li>if $h_w(x)=0$ and $y=0$, cost $=0$</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><img src="/ml_tutorial/images/cost_0.svg" alt="&quot;Cost Function&quot;">
<em>Fig4. cost function $-log(1-h(x))$ vs $h(x)$ when y=0</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The two cost functions can be combined into a single function:</p>
$$J(w)=-\frac{1}{m}\sum_{i=1}^{m}\left[y^i\log(h_{w}(x^i))+(1-y^i)\log(1-h_{w}(x^i))\right]$$<p>{% include note.html content='$y=0$ or $1$ always' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The cost function $J(w)$ that we drived here is the same function we obtained by using MLE.</p>

</div>
</div>
</div>
</div>
 

