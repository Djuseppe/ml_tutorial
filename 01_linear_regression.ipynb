{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "> This tutorial describes linear regression technique and demonstrates how it works via an example of fitting a curve using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is an approach to modeling the relationship between a real-valued target $y$ (dependent variable) and data points $\\mathbf{x}$ (independent variables). In other words, linear regression describes the relationship between input and output and predicts the output based on the input data.\n",
    "\n",
    ">Note: **x** is a vector of features, i.e. $\\mathbf{x} = <x_1, x_2,\\cdots, x_n>$\n",
    "\n",
    "Examples of linear regression include, predicting the weight from gender, age, and height, or predicting the stock price today from the prices of yesterday.\n",
    "\n",
    "We wish to learn $f:X\\to Y$, where $Y$ is a real number, given $\\{<X^1,y^1>,\\cdots, <X^m,y^m>\\}$.\n",
    "\n",
    "**Approach**\n",
    "\n",
    "1- Choose some parametraized form for $P(Y|X;\\theta)$\n",
    "\n",
    "2- Derive learning algorithms as Maximum Likelihood Estimates (MLE), or Maximum a Posteriori (MAP) estimate for $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we are going to predict the price of houses (in dollars), $y$, based on their area (in square feet), $x$. Therefore, training set = $\\{<x^1,y^1>,\\cdots,<x^m,y^m>\\}$, where:\n",
    "\n",
    "- $m$: number of trainig examples\n",
    "\n",
    "- $x$: input variable\n",
    "\n",
    "- $y$: output variable or target label\n",
    "\n",
    "- $(x^i,y^i)$: is the $i$th training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/linear_regression_ex.png\" width=\"500\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# Generate some random data\n",
    "rng = np.random.RandomState(1)\n",
    "x = rng.rand(40) ** 2\n",
    "y = 10 - 1.0 / (x + 0.1) + rng.randn(40)\n",
    "source = pd.DataFrame({\"square feet\": x, \"price\": y})\n",
    "\n",
    "# Define the degree of the polynomial fits\n",
    "degree_list = [1, 3, 5, 9]\n",
    "\n",
    "base = alt.Chart(source).mark_circle(color=\"black\").encode(\n",
    "        alt.X(\"square feet\", axis=alt.Axis(labels=False)), \n",
    "        alt.Y(\"price\", axis=alt.Axis(labels=False))\n",
    ")\n",
    "\n",
    "polynomial_fit = [\n",
    "    base.transform_regression(\n",
    "        \"square feet\", \"price\", method=\"poly\", order=order, as_=[\"square feet\", str(order)]\n",
    "    )\n",
    "    .mark_line()\n",
    "    .transform_fold([str(order)], as_=[\"degree\", \"price\"])\n",
    "    .encode(alt.Color(\"degree:N\"))\n",
    "    for order in degree_list\n",
    "]\n",
    "\n",
    "# chart = alt.layer(base, *polynomial_fit)\n",
    "# chart.save(\"lr_house_ex.png\", scale_factor=2)\n",
    "# chart = alt.layer(base,polynomial_fit[0])\n",
    "# chart.save(\"lr_house_line_ex.png\", scale_factor=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may think of a line as the regression model to predict the prices of houses from their area.\n",
    "\n",
    "\n",
    "<img src=\"images/lr_house_line_ex.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "\n",
    "Someone might consider a quadratic function or even high order polynomial functions to do the prediction.\n",
    "\n",
    "\n",
    "<img src=\"images/lr_house_ex.png\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression Model\n",
    "\n",
    "If we represent the relationship between the independent variable $x$ and dependent variable $y$ as an $n$th degree polynomial, then the regression model is called **polynomial regression**. Therefore, we have:\n",
    "\n",
    "$$y_i=w_0+w_1x_i+w_2x_i^2+\\cdots+w_nx_i^n+\\epsilon_i$$\n",
    "\n",
    "We treat each $x_i^p$ for $p=1,\\cdots,n$ as a different feature. i.e. $feature\\ 1=x, feature\\ 2=x^2,\\cdots, feature\\ n=x^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Expansion\n",
    "\n",
    "In general, if we have a dataset of $m$ instances with $k$ features, $\\mathbf{x}=<x_1,x_2,\\cdots,x_k>$ and a real valued target $y$, then the linear regression model takes the form:\n",
    "\n",
    "$$y_i=w_0+w_1h_1(x_1)+w_2h_2(x_2)+\\cdots+w_kh_k(x_k)+\\epsilon_i= w_0 +\\sum_{j=1}^{k}w_jh_j(x_i)+\\epsilon_i$$\n",
    "\n",
    "\n",
    "Considering our house price prediction, instead of using only area of the house, we can add more features such as number of bathrooms, number of bedrooms, age of the house, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Regression Problem\n",
    "\n",
    "The general approach is as follows:\n",
    "\n",
    "- Instances: $<\\mathbf{x}_i,y_i>$\n",
    "\n",
    "- Learn: mapping from $\\mathbf{x}$ to $y\\equiv f(\\mathbf{x})$\n",
    "\n",
    "- Given the basis functions $h_1, h_2,\\cdots,h_k$ where $h_j(\\mathbf{x})\\in \\mathbb{R}$\n",
    "\n",
    "- Find coeffcients $\\mathbf{x}=[w_0,w_1,\\cdots,w_k]$, where $f(\\mathbf{x})\\thickapprox \\hat{f}(\\mathbf{x})=w_0+\\sum_{j}w_jh_j(\\mathbf{x})$\n",
    "\n",
    "- In order to find coefficients, we need to minimize the **mean residual square error**:\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{m}\\sum_{i=1}^{m}\\left( f(\\mathbf{x}_i) - \\left(w_0 +\\sum_{j=1}^{k}w_jh_j(\\mathbf{x})\\right)\\right)^2$$\n",
    "\n",
    "\n",
    "$$\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\arg\\min}\\ J(\\mathbf{w})$$\n",
    "\n",
    "\n",
    ">Note: The reason it's called *linear* regression is that the model is a linear combinations of the input features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Fit a Line to Two Diminsional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to find a **line** that **fits** our training examples the **best**.\n",
    "\n",
    "<img src=\"images/linear_regression_ex_model.png\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are looling for a line to fit our training set, we need to find a function $\\hat y = f(x)$ that estimates $y^i$ for all $1\\leq i \\leq m$. Thus, we have:\n",
    "\n",
    "$\\hat y = f(x) = w_0 + w_1x$\n",
    "\n",
    "$w_0$ and $w_1$ are the paramaters that we need to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (Error Function)\n",
    "\n",
    "We wish to estimate the actual $y$, i.e. we want to minimize the error as much as possible. The error is the difference between the actual $y^i$ and our estimated/predicted $\\hat{y}^i$ for all $1\\leq i \\leq m$. \n",
    "\n",
    "The error/cost function is:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(w_0,w_1) = \\frac{1}{m}\\sum_{i=1}^{m}\\left( y^i - (w_0 + w_1 x^i)\\right)^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisely, we need to minimize the **mean residual squared error**:\n",
    "\n",
    "$$\\text{minimize } J(w_0,w_1) = \\underset{\\mathbf{w0,w_1}}{\\arg\\min} \\frac{1}{m}\\sum_{i=1}^{m}\\left( \\hat{y}^i - (w_0 + w_1 x^i)\\right)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Linear Regression Model using Gradient Descent Algorithm\n",
    "\n",
    "\n",
    "In order to mininze the cost function, we need to take the gradient (i.e. derivative) of the function with resptect to our parameters $w_0$ and $w_1$, set it zero and solve for $w_0$ and $w_1$.\n",
    " \n",
    " \n",
    "$$\\frac{\\partial{J}}{\\partial{w_0}} = \\frac{-2}{m} \\sum_{i=1}^{m}\\left(\\hat{y}^i - (w_0 + w_1 x^i)\\right)$$\n",
    "\n",
    "$$\\frac{\\partial{J}}{\\partial{w_1}} = \\frac{-2}{m} \\sum_{i=1}^{m}x^i\\left(\\hat{y}^i - (w_0 + w_1 x^i)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gradient descent algorithm:\n",
    "\n",
    "initialize $w_0^{(0)} = w_1^{(0)} = 0$, for $t=0$\n",
    "\n",
    "for $t=1$ to *num_iterations*\n",
    "   \n",
    "\n",
    "   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$w_0^{(t+1)} = w_0^{(t)} - \\eta \\frac{-2}{m} \\sum_{i=1}^{m}\\left(\\hat{y}^i - (w_0 + w_1 x^i)\\right) $\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$w_1^{(t+1)} = w_1^{(t)} - \\eta\\frac{-2}{m} \\sum_{i=1}^{m}x^i\\left(\\hat{y}^i - (w_0 + w_1 x^i)\\right)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$t = t+1$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "def generate_data():\n",
    "    \"\"\"It generates dummy data.\"\"\"\n",
    "    noise = np.random.randn(100,1)\n",
    "    X = 2 * np.random.rand(100,1)\n",
    "    y = 5 + 3 * X + noise\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_point_plot(x,y):\n",
    "    \"\"\"It plots the point chart of the data\"\"\"\n",
    "    \n",
    "    data_points = pd.DataFrame({'x': x.flatten(), 'y': y.flatten()})\n",
    "    chart = alt.Chart(data_points).mark_point(size=50, color='red',filled=True).encode(\n",
    "        x=\"x\",\n",
    "        y=\"y\"\n",
    "    )\n",
    "    return chart\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_line_plot(x,y):\n",
    "    \"\"\"It plots the line chart of the data\"\"\"\n",
    "    \n",
    "    data = pd.DataFrame({'x': x.flatten(), 'y': y.flatten()})\n",
    "    line = alt.Chart(data).mark_line(size=3).encode(\n",
    "        x=\"x\",\n",
    "        y=\"y\"\n",
    "    )\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gradient_descent(data,w_0_t,w_1_t,learning_rate,num_iterations):\n",
    "    \"\"\"Gradient descent implementation, which gets `data`, starting `w_0` and `w_1`, `learning_rate` \n",
    "    and the number of iterations `num_iterations`\"\"\"\n",
    "    \n",
    "    w_0 = 0\n",
    "    w_1 = 0\n",
    "    (X,y) = data\n",
    "    N = len(X)\n",
    "    w_0_t = 0\n",
    "    w_1_t = 0\n",
    "    for t in range(0,num_iterations):\n",
    "        w_0_deriv = np.zeros((N,N))\n",
    "        w_1_deriv = np.zeros((N,N))\n",
    "        w_0_deriv = -2 * (y - (w_0_t + w_1_t * X))\n",
    "        w_1_deriv = -2 * np.dot(X.T, (y - (w_0_t + w_1_t * X)))\n",
    "        w0_sum = np.sum(w_0_deriv,axis=0)\n",
    "        w1_sum = np.sum(w_1_deriv,axis=0)\n",
    "        w_0 = w_0 - learning_rate * (w0_sum / N)\n",
    "        w_1 = w_1 - learning_rate * (w1_sum / N)\n",
    "        w_0_t = w_0\n",
    "        w_1_t = w_1\n",
    "    return w_0, w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an Example\n",
    "\n",
    "In this example, we generate some linear-looking data and then find the line that fits the data. The function that we use to generate the data is $y=5+3x+\\text{Gaussian noise}$. Our goal is to find $\\theta=[w_0,w_1]$ where $w_0=5$ and $w_1=3$ or close enough, as we have noise and it makes it impossible to recover the exact paratmeters of the function. We use the `generate_data` function to generate the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart below shows the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ac1ba4d1fec74e0a82571a92a20dd31e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-ac1ba4d1fec74e0a82571a92a20dd31e\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-9472fd62db5b5b4a965a3d13efb3049e\"}, \"mark\": {\"type\": \"point\", \"color\": \"red\", \"filled\": true, \"size\": 50}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-9472fd62db5b5b4a965a3d13efb3049e\": [{\"x\": 1.6548345977426602, \"y\": 12.536204930191898}, {\"x\": 1.3410145571834204, \"y\": 9.085715672373384}, {\"x\": 0.660663654673936, \"y\": 8.623365745366533}, {\"x\": 0.7321176303485462, \"y\": 7.717521955121958}, {\"x\": 1.6283708340783167, \"y\": 9.963783838290992}, {\"x\": 0.8208160043349912, \"y\": 6.3742749206811125}, {\"x\": 1.4188622554566255, \"y\": 8.717582002811591}, {\"x\": 1.341214230009343, \"y\": 10.313761267045313}, {\"x\": 1.6432269870652119, \"y\": 9.1192022014591}, {\"x\": 1.1036875905027528, \"y\": 9.410650757891633}, {\"x\": 1.3892142495624187, \"y\": 9.905473936775872}, {\"x\": 0.5688731974627856, \"y\": 8.43073687885092}, {\"x\": 1.9809517571391133, \"y\": 11.124742122085237}, {\"x\": 0.5647247292475861, \"y\": 6.0850466619499946}, {\"x\": 1.585423499378837, \"y\": 7.672970128749102}, {\"x\": 1.8061556061692472, \"y\": 10.061250719376199}, {\"x\": 0.7206523723244842, \"y\": 5.687272255731169}, {\"x\": 0.25247787169191294, \"y\": 4.875246885405642}, {\"x\": 0.1890085559301844, \"y\": 5.66690027907341}, {\"x\": 0.40159950120201615, \"y\": 8.47389904215489}, {\"x\": 1.822988501397723, \"y\": 9.901144379638191}, {\"x\": 1.7807483063010532, \"y\": 10.02468918363531}, {\"x\": 1.2445779785394946, \"y\": 8.597195817620875}, {\"x\": 1.1011058121453428, \"y\": 9.97783133721548}, {\"x\": 0.9094392379425531, \"y\": 8.10395916946068}, {\"x\": 1.1295558332115434, \"y\": 7.537042552533499}, {\"x\": 0.6279962734730822, \"y\": 5.443477007341004}, {\"x\": 1.0071980775837694, \"y\": 7.902502213175998}, {\"x\": 0.9363016963940369, \"y\": 6.494174990614892}, {\"x\": 0.1912223988850561, \"y\": 6.515560804365316}, {\"x\": 0.8374700199829794, \"y\": 8.28134622106283}, {\"x\": 1.6512845903681304, \"y\": 8.968053457513388}, {\"x\": 1.6790362033629254, \"y\": 9.897659642038436}, {\"x\": 0.06427449715598876, \"y\": 5.970911948465751}, {\"x\": 0.12422368871124889, \"y\": 5.970666861950409}, {\"x\": 1.0717936249307096, \"y\": 8.604705276103166}, {\"x\": 1.617155016193145, \"y\": 9.547055731806465}, {\"x\": 1.706296938546021, \"y\": 10.33255608351667}, {\"x\": 1.458497836433772, \"y\": 8.489726697295726}, {\"x\": 0.7390429192248855, \"y\": 6.788829438639937}, {\"x\": 0.25131748567198, \"y\": 5.843630321003735}, {\"x\": 1.513658987476687, \"y\": 10.094159377516515}, {\"x\": 0.9243575928603083, \"y\": 8.444027969286077}, {\"x\": 1.4403255664479284, \"y\": 9.56467026177424}, {\"x\": 0.2829961205788263, \"y\": 5.269733401016485}, {\"x\": 0.36129832061729417, \"y\": 6.9930667636994315}, {\"x\": 1.8149224727695672, \"y\": 9.911031696566235}, {\"x\": 1.170804676018358, \"y\": 8.578953820623727}, {\"x\": 1.1623389650100762, \"y\": 7.74059645109394}, {\"x\": 1.0766067163101674, \"y\": 7.867053967263506}, {\"x\": 1.4445947370307926, \"y\": 9.233437161370192}, {\"x\": 0.04103945679292753, \"y\": 4.219728100162778}, {\"x\": 0.8605260722655377, \"y\": 6.491394909465935}, {\"x\": 0.9067969319295459, \"y\": 6.84723384616766}, {\"x\": 1.8802209407987203, \"y\": 10.03786100191167}, {\"x\": 1.2617542221848703, \"y\": 8.493469086909743}, {\"x\": 1.264654907880162, \"y\": 9.22968136138933}, {\"x\": 0.14865906181435462, \"y\": 4.3488929711851725}, {\"x\": 0.9614738341452791, \"y\": 8.352059657799291}, {\"x\": 0.8913094396561207, \"y\": 7.19337858123152}, {\"x\": 1.6118449748967891, \"y\": 11.565446919797411}, {\"x\": 1.387838484250046, \"y\": 9.859109145825371}, {\"x\": 0.05425601189385021, \"y\": 4.275709511242663}, {\"x\": 0.12778162444289642, \"y\": 5.054150057952032}, {\"x\": 0.003749564452929066, \"y\": 6.078310539502923}, {\"x\": 0.8790638810736446, \"y\": 7.598326286702819}, {\"x\": 1.0747258477977861, \"y\": 8.456774404801527}, {\"x\": 0.6470281123817587, \"y\": 4.412918931661801}, {\"x\": 0.6340245740097517, \"y\": 7.116212569388643}, {\"x\": 0.5118169767170564, \"y\": 6.280902629234343}, {\"x\": 0.0597522012563918, \"y\": 6.128202963962096}, {\"x\": 0.8510564483708709, \"y\": 5.852237710829709}, {\"x\": 0.5007556882335837, \"y\": 5.825442760978802}, {\"x\": 1.1542121293495193, \"y\": 9.05829442705037}, {\"x\": 1.9557256302970338, \"y\": 10.671152489570147}, {\"x\": 0.600405529570188, \"y\": 6.911263396169721}, {\"x\": 0.4059055249589678, \"y\": 6.655900550401958}, {\"x\": 1.118066368803038, \"y\": 8.01315084615812}, {\"x\": 1.1732980672378743, \"y\": 8.245151253621984}, {\"x\": 0.8568411034650254, \"y\": 4.56538941842414}, {\"x\": 0.028777989726028297, \"y\": 5.187400012805597}, {\"x\": 1.1086470009082308, \"y\": 7.568664374118383}, {\"x\": 1.3327598786387787, \"y\": 9.30152080731157}, {\"x\": 0.8868820667427197, \"y\": 8.941934991320553}, {\"x\": 1.605791893025592, \"y\": 10.282984443696053}, {\"x\": 1.2224670127723833, \"y\": 9.293341186001403}, {\"x\": 0.6742024555646289, \"y\": 8.307585221071607}, {\"x\": 1.1910887464930937, \"y\": 7.848791394157305}, {\"x\": 1.439302150530358, \"y\": 10.267006152372291}, {\"x\": 1.0016151941854623, \"y\": 8.663809833639773}, {\"x\": 1.2746993605454673, \"y\": 9.008777254273896}, {\"x\": 0.5530166186964978, \"y\": 6.594439323673822}, {\"x\": 0.13296450408245497, \"y\": 5.712010579977924}, {\"x\": 1.016338626202368, \"y\": 8.32555616893404}, {\"x\": 1.4583890629650613, \"y\": 9.3997902555327}, {\"x\": 0.3242464431418659, \"y\": 6.099340461497045}, {\"x\": 0.22629683159779512, \"y\": 5.702874164535679}, {\"x\": 1.409009055538408, \"y\": 7.62158739983942}, {\"x\": 0.5972796661239299, \"y\": 5.751508859385963}, {\"x\": 0.5351660216724641, \"y\": 7.997971894885552}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_chart = make_point_plot(X,y)\n",
    "\n",
    "#show the chart\n",
    "origin_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the initial parameters to 0:  $w_0^{(0)} = w_1^{(0)} = 0$, define the number of iterations of the graditent descent algorithm as well as the learning rate. Then, we run the `gradient_descent` function. The outputs of this function are the estimated parameters $w_0$ and $w_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_0 = [4.96690133], w_1 = [3.00173485]\n"
     ]
    }
   ],
   "source": [
    "initial_w_0 = 0\n",
    "initial_w_1 = 0\n",
    "num_iterations = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# X,y = generate_data()\n",
    "data = (X,y)\n",
    "w_0,w_1 = gradient_descent(data,initial_w_0,initial_w_1,learning_rate,num_iterations)\n",
    "\n",
    "print(\"w_0 = {}, w_1 = {}\".format(w_0,w_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated parameters are $w_0 = 4.966$ and $w_1 = 3.001$ that are close enough to $w_0=5$ and $w_1=3$. Let's plot the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-08b4fb807120433589e17dea402ce7ed\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-08b4fb807120433589e17dea402ce7ed\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-9472fd62db5b5b4a965a3d13efb3049e\"}, \"mark\": {\"type\": \"point\", \"color\": \"red\", \"filled\": true, \"size\": 50}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}}, {\"data\": {\"name\": \"data-c05db3bc4da7ff9e7977325279122a75\"}, \"mark\": {\"type\": \"line\", \"size\": 3}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-9472fd62db5b5b4a965a3d13efb3049e\": [{\"x\": 1.6548345977426602, \"y\": 12.536204930191898}, {\"x\": 1.3410145571834204, \"y\": 9.085715672373384}, {\"x\": 0.660663654673936, \"y\": 8.623365745366533}, {\"x\": 0.7321176303485462, \"y\": 7.717521955121958}, {\"x\": 1.6283708340783167, \"y\": 9.963783838290992}, {\"x\": 0.8208160043349912, \"y\": 6.3742749206811125}, {\"x\": 1.4188622554566255, \"y\": 8.717582002811591}, {\"x\": 1.341214230009343, \"y\": 10.313761267045313}, {\"x\": 1.6432269870652119, \"y\": 9.1192022014591}, {\"x\": 1.1036875905027528, \"y\": 9.410650757891633}, {\"x\": 1.3892142495624187, \"y\": 9.905473936775872}, {\"x\": 0.5688731974627856, \"y\": 8.43073687885092}, {\"x\": 1.9809517571391133, \"y\": 11.124742122085237}, {\"x\": 0.5647247292475861, \"y\": 6.0850466619499946}, {\"x\": 1.585423499378837, \"y\": 7.672970128749102}, {\"x\": 1.8061556061692472, \"y\": 10.061250719376199}, {\"x\": 0.7206523723244842, \"y\": 5.687272255731169}, {\"x\": 0.25247787169191294, \"y\": 4.875246885405642}, {\"x\": 0.1890085559301844, \"y\": 5.66690027907341}, {\"x\": 0.40159950120201615, \"y\": 8.47389904215489}, {\"x\": 1.822988501397723, \"y\": 9.901144379638191}, {\"x\": 1.7807483063010532, \"y\": 10.02468918363531}, {\"x\": 1.2445779785394946, \"y\": 8.597195817620875}, {\"x\": 1.1011058121453428, \"y\": 9.97783133721548}, {\"x\": 0.9094392379425531, \"y\": 8.10395916946068}, {\"x\": 1.1295558332115434, \"y\": 7.537042552533499}, {\"x\": 0.6279962734730822, \"y\": 5.443477007341004}, {\"x\": 1.0071980775837694, \"y\": 7.902502213175998}, {\"x\": 0.9363016963940369, \"y\": 6.494174990614892}, {\"x\": 0.1912223988850561, \"y\": 6.515560804365316}, {\"x\": 0.8374700199829794, \"y\": 8.28134622106283}, {\"x\": 1.6512845903681304, \"y\": 8.968053457513388}, {\"x\": 1.6790362033629254, \"y\": 9.897659642038436}, {\"x\": 0.06427449715598876, \"y\": 5.970911948465751}, {\"x\": 0.12422368871124889, \"y\": 5.970666861950409}, {\"x\": 1.0717936249307096, \"y\": 8.604705276103166}, {\"x\": 1.617155016193145, \"y\": 9.547055731806465}, {\"x\": 1.706296938546021, \"y\": 10.33255608351667}, {\"x\": 1.458497836433772, \"y\": 8.489726697295726}, {\"x\": 0.7390429192248855, \"y\": 6.788829438639937}, {\"x\": 0.25131748567198, \"y\": 5.843630321003735}, {\"x\": 1.513658987476687, \"y\": 10.094159377516515}, {\"x\": 0.9243575928603083, \"y\": 8.444027969286077}, {\"x\": 1.4403255664479284, \"y\": 9.56467026177424}, {\"x\": 0.2829961205788263, \"y\": 5.269733401016485}, {\"x\": 0.36129832061729417, \"y\": 6.9930667636994315}, {\"x\": 1.8149224727695672, \"y\": 9.911031696566235}, {\"x\": 1.170804676018358, \"y\": 8.578953820623727}, {\"x\": 1.1623389650100762, \"y\": 7.74059645109394}, {\"x\": 1.0766067163101674, \"y\": 7.867053967263506}, {\"x\": 1.4445947370307926, \"y\": 9.233437161370192}, {\"x\": 0.04103945679292753, \"y\": 4.219728100162778}, {\"x\": 0.8605260722655377, \"y\": 6.491394909465935}, {\"x\": 0.9067969319295459, \"y\": 6.84723384616766}, {\"x\": 1.8802209407987203, \"y\": 10.03786100191167}, {\"x\": 1.2617542221848703, \"y\": 8.493469086909743}, {\"x\": 1.264654907880162, \"y\": 9.22968136138933}, {\"x\": 0.14865906181435462, \"y\": 4.3488929711851725}, {\"x\": 0.9614738341452791, \"y\": 8.352059657799291}, {\"x\": 0.8913094396561207, \"y\": 7.19337858123152}, {\"x\": 1.6118449748967891, \"y\": 11.565446919797411}, {\"x\": 1.387838484250046, \"y\": 9.859109145825371}, {\"x\": 0.05425601189385021, \"y\": 4.275709511242663}, {\"x\": 0.12778162444289642, \"y\": 5.054150057952032}, {\"x\": 0.003749564452929066, \"y\": 6.078310539502923}, {\"x\": 0.8790638810736446, \"y\": 7.598326286702819}, {\"x\": 1.0747258477977861, \"y\": 8.456774404801527}, {\"x\": 0.6470281123817587, \"y\": 4.412918931661801}, {\"x\": 0.6340245740097517, \"y\": 7.116212569388643}, {\"x\": 0.5118169767170564, \"y\": 6.280902629234343}, {\"x\": 0.0597522012563918, \"y\": 6.128202963962096}, {\"x\": 0.8510564483708709, \"y\": 5.852237710829709}, {\"x\": 0.5007556882335837, \"y\": 5.825442760978802}, {\"x\": 1.1542121293495193, \"y\": 9.05829442705037}, {\"x\": 1.9557256302970338, \"y\": 10.671152489570147}, {\"x\": 0.600405529570188, \"y\": 6.911263396169721}, {\"x\": 0.4059055249589678, \"y\": 6.655900550401958}, {\"x\": 1.118066368803038, \"y\": 8.01315084615812}, {\"x\": 1.1732980672378743, \"y\": 8.245151253621984}, {\"x\": 0.8568411034650254, \"y\": 4.56538941842414}, {\"x\": 0.028777989726028297, \"y\": 5.187400012805597}, {\"x\": 1.1086470009082308, \"y\": 7.568664374118383}, {\"x\": 1.3327598786387787, \"y\": 9.30152080731157}, {\"x\": 0.8868820667427197, \"y\": 8.941934991320553}, {\"x\": 1.605791893025592, \"y\": 10.282984443696053}, {\"x\": 1.2224670127723833, \"y\": 9.293341186001403}, {\"x\": 0.6742024555646289, \"y\": 8.307585221071607}, {\"x\": 1.1910887464930937, \"y\": 7.848791394157305}, {\"x\": 1.439302150530358, \"y\": 10.267006152372291}, {\"x\": 1.0016151941854623, \"y\": 8.663809833639773}, {\"x\": 1.2746993605454673, \"y\": 9.008777254273896}, {\"x\": 0.5530166186964978, \"y\": 6.594439323673822}, {\"x\": 0.13296450408245497, \"y\": 5.712010579977924}, {\"x\": 1.016338626202368, \"y\": 8.32555616893404}, {\"x\": 1.4583890629650613, \"y\": 9.3997902555327}, {\"x\": 0.3242464431418659, \"y\": 6.099340461497045}, {\"x\": 0.22629683159779512, \"y\": 5.702874164535679}, {\"x\": 1.409009055538408, \"y\": 7.62158739983942}, {\"x\": 0.5972796661239299, \"y\": 5.751508859385963}, {\"x\": 0.5351660216724641, \"y\": 7.997971894885552}], \"data-c05db3bc4da7ff9e7977325279122a75\": [{\"x\": 1.6548345977426602, \"y\": 10.058680489283764}, {\"x\": 1.3410145571834204, \"y\": 9.043681387234281}, {\"x\": 0.660663654673936, \"y\": 6.843198735154175}, {\"x\": 0.7321176303485462, \"y\": 7.074304820725866}, {\"x\": 1.6283708340783167, \"y\": 9.973087815507126}, {\"x\": 0.8208160043349912, \"y\": 7.36118506517761}, {\"x\": 1.4188622554566255, \"y\": 9.295466912795726}, {\"x\": 1.341214230009343, \"y\": 9.044327196019005}, {\"x\": 1.6432269870652119, \"y\": 10.021137589239668}, {\"x\": 1.1036875905027528, \"y\": 8.276086501822451}, {\"x\": 1.3892142495624187, \"y\": 9.199575333320588}, {\"x\": 0.5688731974627856, \"y\": 6.546317658088474}, {\"x\": 1.9809517571391133, \"y\": 11.113452591665936}, {\"x\": 0.5647247292475861, \"y\": 6.532900122657006}, {\"x\": 1.585423499378837, \"y\": 9.83418175304627}, {\"x\": 1.8061556061692472, \"y\": 10.548103304348423}, {\"x\": 0.7206523723244842, \"y\": 7.037222336833343}, {\"x\": 0.25247787169191294, \"y\": 5.522989221262694}, {\"x\": 0.1890085559301844, \"y\": 5.317708199722297}, {\"x\": 0.40159950120201615, \"y\": 6.005298508391661}, {\"x\": 1.822988501397723, \"y\": 10.60254652445428}, {\"x\": 1.7807483063010532, \"y\": 10.465927588273175}, {\"x\": 1.2445779785394946, \"y\": 8.731773197579223}, {\"x\": 1.1011058121453428, \"y\": 8.267736166038311}, {\"x\": 0.9094392379425531, \"y\": 7.647822280481316}, {\"x\": 1.1295558332115434, \"y\": 8.359753061403003}, {\"x\": 0.6279962734730822, \"y\": 6.737541484820235}, {\"x\": 1.0071980775837694, \"y\": 7.964007104995426}, {\"x\": 0.9363016963940369, \"y\": 7.734704466712985}, {\"x\": 0.1912223988850561, \"y\": 5.324868509201797}, {\"x\": 0.8374700199829794, \"y\": 7.415049728817646}, {\"x\": 1.6512845903681304, \"y\": 10.047198576621348}, {\"x\": 1.6790362033629254, \"y\": 10.136956586402832}, {\"x\": 0.06427449715598876, \"y\": 4.914276483171688}, {\"x\": 0.12422368871124889, \"y\": 5.108172244222343}, {\"x\": 1.0717936249307096, \"y\": 8.172930736630617}, {\"x\": 1.617155016193145, \"y\": 9.936812104555356}, {\"x\": 1.706296938546021, \"y\": 10.225126932957956}, {\"x\": 1.458497836433772, \"y\": 9.423661654690864}, {\"x\": 0.7390429192248855, \"y\": 7.096703524066772}, {\"x\": 0.25131748567198, \"y\": 5.519236144288502}, {\"x\": 1.513658987476687, \"y\": 9.602071289323504}, {\"x\": 0.9243575928603083, \"y\": 7.696073236088209}, {\"x\": 1.4403255664479284, \"y\": 9.364886448094872}, {\"x\": 0.2829961205788263, \"y\": 5.621695457999963}, {\"x\": 0.36129832061729417, \"y\": 5.874950994472119}, {\"x\": 1.8149224727695672, \"y\": 10.57645828674934}, {\"x\": 1.170804676018358, \"y\": 8.493165632298371}, {\"x\": 1.1623389650100762, \"y\": 8.465784687931963}, {\"x\": 1.0766067163101674, \"y\": 8.188497885941814}, {\"x\": 1.4445947370307926, \"y\": 9.378694375403725}, {\"x\": 0.04103945679292753, \"y\": 4.839126581774174}, {\"x\": 0.8605260722655377, \"y\": 7.489620722823954}, {\"x\": 0.9067969319295459, \"y\": 7.639276177989701}, {\"x\": 1.8802209407987203, \"y\": 10.787655399262254}, {\"x\": 1.2617542221848703, \"y\": 8.787326921444569}, {\"x\": 1.264654907880162, \"y\": 8.796708710353915}, {\"x\": 0.14865906181435462, \"y\": 5.187204423664098}, {\"x\": 0.9614738341452791, \"y\": 7.816119589751802}, {\"x\": 0.8913094396561207, \"y\": 7.589184441583984}, {\"x\": 1.6118449748967891, \"y\": 9.919637652796181}, {\"x\": 1.387838484250046, \"y\": 9.195125647589233}, {\"x\": 0.05425601189385021, \"y\": 4.881873346881075}, {\"x\": 0.12778162444289642, \"y\": 5.119679799846696}, {\"x\": 0.003749564452929066, \"y\": 4.718518582447007}, {\"x\": 0.8790638810736446, \"y\": 7.54957820438142}, {\"x\": 1.0747258477977861, \"y\": 8.182414527314563}, {\"x\": 0.6470281123817587, \"y\": 6.799096825161893}, {\"x\": 0.6340245740097517, \"y\": 6.757039027490551}, {\"x\": 0.5118169767170564, \"y\": 6.36177873342308}, {\"x\": 0.0597522012563918, \"y\": 4.899649863822862}, {\"x\": 0.8510564483708709, \"y\": 7.458992787996387}, {\"x\": 0.5007556882335837, \"y\": 6.326002822304632}, {\"x\": 1.1542121293495193, \"y\": 8.439499779921405}, {\"x\": 1.9557256302970338, \"y\": 11.03186284982769}, {\"x\": 0.600405529570188, \"y\": 6.648303779553199}, {\"x\": 0.4059055249589678, \"y\": 6.0192256312067665}, {\"x\": 1.118066368803038, \"y\": 8.322592285957047}, {\"x\": 1.1732980672378743, \"y\": 8.501230094479808}, {\"x\": 0.8568411034650254, \"y\": 7.477702299714535}, {\"x\": 0.028777989726028297, \"y\": 4.799468891205612}, {\"x\": 1.1086470009082308, \"y\": 8.292126895862566}, {\"x\": 1.3327598786387787, \"y\": 9.016982992525048}, {\"x\": 0.8868820667427197, \"y\": 7.574864834959971}, {\"x\": 1.605791893025592, \"y\": 9.900059958991324}, {\"x\": 1.2224670127723833, \"y\": 8.660258929850906}, {\"x\": 0.6742024555646289, \"y\": 6.886987751057657}, {\"x\": 1.1910887464930937, \"y\": 8.558771108867287}, {\"x\": 1.439302150530358, \"y\": 9.361576378299755}, {\"x\": 1.0016151941854623, \"y\": 7.9459501905101995}, {\"x\": 1.2746993605454673, \"y\": 8.829195833922235}, {\"x\": 0.5530166186964978, \"y\": 6.495032172366535}, {\"x\": 0.13296450408245497, \"y\": 5.136442968224368}, {\"x\": 1.016338626202368, \"y\": 7.993570700180692}, {\"x\": 1.4583890629650613, \"y\": 9.4233098448674}, {\"x\": 0.3242464431418659, \"y\": 5.755112814922499}, {\"x\": 0.22629683159779512, \"y\": 5.438310970189577}, {\"x\": 1.409009055538408, \"y\": 9.263598364657492}, {\"x\": 0.5972796661239299, \"y\": 6.638193690390546}, {\"x\": 0.5351660216724641, \"y\": 6.437297363849674}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = w_0 + w_1 * X\n",
    "line = make_line_plot(X,y_predicted)\n",
    "\n",
    "#show the charts\n",
    "origin_chart + line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "num_iterations = 1000\n",
    "l_rates = [0.001]\n",
    "learning_rate = 0.001\n",
    "all_params = []\n",
    "\n",
    "# X,y = generate_data()\n",
    "data = (X,y)\n",
    "for i in range(0,len(l_rates)):\n",
    "    initial_w_0 = 0\n",
    "    initial_w_1 = 0\n",
    "    learning_rate = l_rates[i]\n",
    "    w_0,w_1 = gradient_descent(data,initial_w_0,initial_w_1,learning_rate,num_iterations)\n",
    "    all_params.append((w_0,w_1))\n",
    "#     print(\"learning_rate: {:.4f} => w_0 = {}, w_1 = {}\".format(learning_rate,w_0,w_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
