{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "> Summary: Classification, Logistic Regression, Gradient Descent, Overfitting, Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "Logistic regression is a classification technique used for binary classification problems such as classifying tumors as malignant / not malignant, classifying emails as spam / not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "**Classification** is a learning algorithm that determines which discrete category a new example (instance) belongs, given a set of training instances $X$ with their observed categories $Y$. Binary classification is a classification task where $Y$ has two possible values $0,1$. If $Y$ has more than two possible values, it is called a multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use Linear Regression for classification problems?\n",
    "Thare are two main issues with using linear regression for classification:\n",
    "\n",
    "1. Linear regression outputs values for $Y$ that can be much larger than 1 or much lower than 0, but our classes are 0 and 1.\n",
    "\n",
    "2. Our hypothesis or prediction rule can change each time a new training example arrives, which shouldn't. Instead, we should be able to use the learned hypothesis to make correct predictions for the data we haven't seen before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "If we have consider Naive Bayes assumption, then we have:\n",
    "$$P(X_1\\cdots X_d|Y)=\\prod_{i=1}^{d}P(X_i|Y)$$\n",
    "\n",
    "We also assume parametric form for $P(X_i|Y)$ and $P(Y)$. Then, we use MLE or MAP to estimate the parameters.\n",
    "\n",
    "At last, Naive Bayes classifier for a $X^{new} = <X_1,X_2,\\cdots X_d>$ is:\n",
    "\n",
    "$$Y^{new} =\\underset{y}{\\arg\\max}\\quad P(Y=y_k)\\prod_i P(X^{new}_i|Y=y_K)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative vs Discriminative Classifiers\n",
    "\n",
    "Training a classifier involves estimating $P(Y|X)$.\n",
    "\n",
    "i. Generative classifiers (e.g. Naive Bayes):\n",
    "\n",
    "- Assume some functional form for $P(X,Y)$, i.e. $P(X|Y)$ and $P(Y)$\n",
    "- Estimate parameters of $P(X|Y)$, $P(Y)$ directly from training data\n",
    "- $\\underset{y}{\\arg\\max}\\quad P(X|Y)P(Y)= \\underset{y}{\\arg\\max}\\quad P(Y|X)$\n",
    "\n",
    "**Question:** _Can we learn the $P(Y|X)$ directly from data? Or better yet, can we learn decision boundary directly?_\n",
    "\n",
    "ii. Discriminative classifiers (e.g. Logistic Regression):\n",
    "\n",
    "- Assume some functional form for $P(Y|X)$ or for the decision boundary\n",
    "- Estimate parameters of $P(Y|X)$ directly from training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a classification method for binary classification problems, where input $X$ is a vector of discrete or real-valued variables and $Y$ is discrete (boolean valued). The idea is to learn $P(Y|X)$ directly from observed data.\n",
    "\n",
    "Let's consider learning $f:X\\rightarrow Y$ where,\n",
    "\n",
    "- $X$ is a vector of real-valued features, $<X_1,\\cdots,X_n>$\n",
    "- $Y$ is boolean\n",
    "- Assume all $X_i$ are conditionally independent given $Y$\n",
    "- Assume $P(X_i|Y=y_k) \\sim N(\\mu_{ik},\\sigma_i)$\n",
    "- Assume $P(Y) \\sim $ Bernoulli($\\pi$)\n",
    "\n",
    "What does this imply about the form of $P(Y|X)$?\n",
    "\n",
    "$$P(Y=0|X=<X_1,\\cdots,X_n>)=\\frac{1}{1+exp(w_0+\\sum_{i}w_iX_i)}$$\n",
    "\n",
    "\n",
    "\n",
    "$$P(Y=1|X) = 1 - P(Y=1|X) = \\frac{\\exp(w_0+\\sum_{i=1}^{n}w_iX_i)}{1+\\exp(w_0+\\sum_{i=1}^{n}w_iX_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "In Logistic regression $P(Y|X)$ follows the form of the sigmoid function, which means logistic regression gives the *probability* that an instance belongs to class $1$ or class $0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "x = np.linspace(-10,10)\n",
    "y = sigmoid(x)\n",
    "source = pd.DataFrame({\"x\":x,\"y\":y})\n",
    "base = alt.Chart(source).mark_line(size=3).encode(\n",
    "    x=\"x\",\n",
    "    y=\"y\"\n",
    ")\n",
    "a = pd.DataFrame({\"x\":[0]})\n",
    "vertline = alt.Chart(a).mark_rule(size=2).encode(\n",
    "    x=\"x\"\n",
    ")\n",
    "# base + vertline\n",
    "# base.save(\"sigmoid.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid](images/sigmoid.svg)\n",
    "*Fig1. Sigmoid Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression is Linear\n",
    "\n",
    "The reason that logistic regression is linear is that, the outcome is a linear combinations of the inputs and parameters.\n",
    "\n",
    "$$\\frac{P(Y=1|X)}{P(Y=0|X)} = \\exp(w_0+\\sum_{i=1}^{n}w_iX_i)$$\n",
    "\n",
    "which implies:\n",
    "\n",
    "$$\\ln\\frac{P(Y=1|X)}{P(Y=0|X)} = w_0+\\sum_{i=1}^{n}w_iX_i$$\n",
    "\n",
    "where $w_0+\\sum_{i=1}^{n}w_iX_i$ is the linear classification rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary\n",
    "\n",
    "Since logistic regression prediction function returns a probability between 0 and 1, in order to predict which class this data belongs we need to set a threshold. For each data point, if the estimated probability is above this threshold, we classify the point into class 1, and if it's below the threshold, we classify the point into class 2.\n",
    "\n",
    "If $P(Y=1|X)\\geq0.5$, class $=1$, and if $P(Y=1|X)<0.5$, class $=0$.\n",
    "\n",
    "> Note: Decision boundary can be linear (a line) or non-linear (a curve or higher order polynomial). Polynomial order can be increased to get complex decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic Regression: MLCE\n",
    "\n",
    "We have a collection of training data $D = \\{<X^1,Y^1>,\\cdots,<X^M,Y^M>\\}$. We need to find the parameters $\\mathbf{w}=<w_0,\\cdots,w_n>$ that **maximize the conditional likelihood** of the training data.\n",
    "\n",
    "Data likelihood $=\\prod_j P(<X^j,Y^j>|\\mathbf{w})$, thus data **conditional** likelihood $=\\prod_j P(Y^j|X^j,\\mathbf{w})$.\n",
    "\n",
    "therefore,\n",
    "$$W_{MLCE}=\\underset{\\mathbf{w}}{\\arg\\max}\\prod_j P(Y^j|X^j,\\mathbf{w})$$\n",
    "\n",
    "In order to make arithmetic easier, we work with the conditional log likelihood. Additionally, we know that maximizing a function is equivalent to *minimizing the negative of the function*. Therefore, we convert our problem to a minimization problem and apply the Gradient Descent algorithm to find the minimum.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  W_{MLCE}&=\\underset{\\mathbf{w}}{\\arg\\max}\\quad \\ln \\prod_j P(Y^j|X^j,\\mathbf{w}) =  \\sum_j \\ln P(Y^j|X^j,\\mathbf{w})\\\\\n",
    "  &=\\underset{W}{\\arg\\max}\\quad\\sum_j \\ln P(Y^j|X^j,\\mathbf{w})\\\\\n",
    "  &=\\underset{W}{\\arg\\min}\\quad -\\sum_j \\ln P(Y^j|X^j,\\mathbf{w})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "if $J(\\mathbf{w})=-\\sum_j \\ln P(Y^j|X^j,\\mathbf{w})$, then we have:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  J(\\mathbf{w})&=-\\sum_j \\left [ Y^j \\ln P(Y^j=1|X^j,\\mathbf{w}) + (1-Y^j) \\ln P(Y^j=0|X^j,\\mathbf{w})\\right ]\\\\\n",
    "  &=-\\sum_j \\left[ Y^j(w_0+\\sum_iw_iX_i^j)-\\ln(1+\\exp(w_0+\\sum_iw_iX_i^j))\\right ]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$J(\\mathbf{w})$ is a convex function, so we can always find global minimum. There is no closed-form solution to minimize it. However, we can use gradient descent algorithm to find the minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent algorithm for Logistic Regression\n",
    "\n",
    "1. Compute the gradient of $J_D(\\mathbf{w})$ over the entire training set $D$:\n",
    "$$\\nabla J_D(\\mathbf{w}) = \\left [\\frac{\\partial J_D(\\mathbf{w})}{\\partial w_0},\\cdots \\frac{\\partial J_D(\\mathbf{w})}{\\partial w_n} \\right]$$\n",
    "\n",
    "$$\\frac{\\partial J_D(\\mathbf{w})}{\\partial w_i}=\\sum_{j=1}^{M}X_i^j\\left[Y^j-\\hat{P}(Y^j=1|X^j,\\mathbf{w})\\right]$$\n",
    "\n",
    "We assume $X_0^j=1$, (for $j=0,1,\\cdots M$)\n",
    "\n",
    "2. Do until satisfied:\n",
    "   - Update the vector of parameters: $w_i=w_i-\\eta \\frac{\\partial J_D(\\mathbf{w})}{\\partial w_i}$, (for $i=0,1,\\cdots n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Maximum a Posteriori (MAP) to Estimate Parameters\n",
    "We assume Gaussian distributions for the prior: $\\mathbf{w} \\sim N(0,\\sigma I)$.Thus,\n",
    "\n",
    "$$\\mathbf{w^*}=\\underset{\\mathbf{w}}{\\arg\\max}\\quad \\ln\\left[P(\\mathbf{w})\\prod_j P(Y^j|X^j,\\mathbf{w})\\right]$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$J(\\mathbf{w})=-\\sum_j \\left[ Y^j \\ln P(Y^j=1|X^j,\\mathbf{w}) + (1-Y^j) \\ln P(Y^j=0|X^j,\\mathbf{w})\\right]+\\lambda\\sum_{i=1}^{n}w_i^2$$\n",
    "\n",
    "where $\\lambda \\sum_{i=1}^{n}w_i^2$ is called a **regularization** term. Regularization helps reduce the overfitting, and also keeps the weights near to zero (i.e. discourage the weights from getting large values).\n",
    "\n",
    "\n",
    "And the modified gradient descent rule is:\n",
    "\n",
    "$$w_i=w_i-\\eta \\left [\\frac{\\partial J_D(\\mathbf{w})}{\\partial w_i} + \\lambda w_i \\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Fitting using Cost Function\n",
    "\n",
    "In the first part above, we learned how to fit parameters for logistic regression model using two primary principles: (a) maximum likelihood (conditional) estimates (MLE), and (b) map a posteriori estimation. In this section, we show how to train a logistic regression model (i.e. find the parameters $w$) by defining and minimizing a **cost** function.\n",
    "\n",
    "In general, while training a classification model, our goal is to find a model that minimizes error. For logistic regression, we would like to find the parameters $w$ that minimize the number of misclassifications. Therefore, we define a cost function based on the parameters and find the set of parameters that give the minimum cost/error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function for Logistic Regression\n",
    "\n",
    "*A cost function primarily measures how wrong a machine learning model is in terms of estimating the relationship between $X$ and $y$.* Therefore, a cost function aims to penalize bad choices for the parameters to be optimized.\n",
    "\n",
    "We can not use the linear regression cost function for logistic regression. If we make use of mean squared error for logistic regression, we will end up with a non-convex function of parameters $w$ (due to non-linear sigmoid function). Thus, gradient descent *cannot* guarantee that it finds the global minimum.\n",
    "\n",
    "\n",
    "![convex](images/con_nonconvex.png)\n",
    "*Fig2. Convex and non-convex functions. ([image source](\"https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\"))*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of Mean Squared Error, we use a cost function called **Cross-entropy** (or Log Loss). The cross-entropy cost functions can be divided into two functions: one for $y=0$ and one for $y=1$.\n",
    "\n",
    "Training data:  $D = \\{<x^1,y^1>,\\cdots,<x^m,y^m>\\}$.\n",
    "\n",
    "Every instance has $n+1$ features. $x=<x_0,\\cdots,x_n>$, where $x_0=1$\n",
    "\n",
    "$w$ is the vector of parameters: $w=<w_0,\\cdots,w_n>$\n",
    "\n",
    "And, $h_{w}(x) = P(y=1|x,w)=\\dfrac{1}{1+e^{-wx}}$\n",
    "\n",
    "The cost functions for each data point $i$ are:\n",
    "- $J_{y=1}^i(w)=-\\log(h_{w}(x^i))$ if $y=1$\n",
    "- $J_{y=0}^i(w)=-\\log(1-h_{w}(x^i))$ if $y=0$\n",
    "\n",
    "We can interpret cost functions as follows:\n",
    "\n",
    "1. if $y=1$:\n",
    "    - if $h_w(x)=0$ and $y=1$, cost is very large (infinite)\n",
    "    - if $h_w(x)=1$ and $y=1$, cost $=0$\n",
    "\n",
    "\n",
    "![\"cost Function\"](images/cost_1.svg)\n",
    "*Fig3. cost function $-log(h(x))$ vs $h(x)$ when y=1*\n",
    "\n",
    "\n",
    "2. if $y=0$:\n",
    "   - if $h_w(x)=1$ and $y=0$, cost is very large (infinite)\n",
    "    - if $h_w(x)=0$ and $y=0$, cost $=0$\n",
    "    \n",
    "\n",
    "![\"Cost Function\"](images/cost_0.svg)\n",
    "*Fig4. cost function $-log(1-h(x))$ vs $h(x)$ when y=0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two cost functions can be combined into a single function:\n",
    "\n",
    "$$J(w)=-\\frac{1}{m}\\sum_{i=1}^{m}\\left[y^i\\log(h_{w}(x^i))+(1-y^i)\\log(1-h_{w}(x^i))\\right]$$\n",
    "> Note: $y=0$ or $1$ always"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function $J(w)$ that we drived here is the same function we obtained by using MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "x = np.linspace(-10,10)\n",
    "h = sigmoid(x)\n",
    "j = -np.log(h)\n",
    "source = pd.DataFrame({\"x\":h,\"y\":j})\n",
    "base = alt.Chart(source).mark_line(size=3).encode(\n",
    "    alt.X(\"x\",title=\"h(x)\",scale=alt.Scale(domain=[-0.01,1])),\n",
    "    alt.Y(\"y\", title=\"cost\",scale=alt.Scale(domain=[0,10]))\n",
    ")\n",
    "base.save(\"images/cost_1.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "x = np.linspace(-10,10)\n",
    "h = sigmoid(x)\n",
    "j = -np.log(1-h)\n",
    "source = pd.DataFrame({\"x\":h,\"y\":j})\n",
    "base = alt.Chart(source).mark_line(size=3).encode(\n",
    "    alt.X(\"x\",title=\"h(x)\",scale=alt.Scale(domain=[0,1.01])),\n",
    "    alt.Y(\"y\", title=\"cost\",scale=alt.Scale(domain=[0,10]))\n",
    ")\n",
    "base.save(\"images/cost_0.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
