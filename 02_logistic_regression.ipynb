{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "> Summary: Classification, Logistic Regression, Gradient Descent, Overfitting, Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "Logistic regression is a classification technique used for binary classification problems such as classifying tumors as malignant / not malignant, classifying emails as spam / not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "**Classification** is a learning algorithm that determines which discrete category a new example (instance) belongs, given a set of training instances $X$ with their observed categories $Y$. Binary classification is a classification task where $Y$ has two possible values $0,1$. If $Y$ has more than two possible values, it is called a multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use Linear Regression for classification problems?\n",
    "Thare are two main issues with using linear regression for classification:\n",
    "\n",
    "1. Linear regression outputs values for $Y$ that can be much larger than 1 or much lower than 0, but our classes are 0 and 1.\n",
    "\n",
    "2. Our hypothesis or prediction rule can change each time a new training example arrives, which shouldn't. Instead, we should be able to use the learned hypothesis to make correct predictions for the data we haven't seen before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "If we have consider Naive Bayes assumption, then we have:\n",
    "$$P(X_1\\cdots X_d|Y)=\\prod_{i=1}^{d}P(X_i|Y)$$\n",
    "\n",
    "We also assume parametric form for $P(X_i|Y)$ and $P(Y)$. Then, we use MLE or MAP to estimate the parameters.\n",
    "\n",
    "At last, Naive Bayes classifier for a $X^{new} = <X_1,X_2,\\cdots X_d>$ is:\n",
    "\n",
    "$$Y^{new} =\\underset{y}{\\arg\\max}\\quad P(Y=y_k)\\prod_i P(X^{new}_i|Y=y_K)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative vs Discriminative Classifiers\n",
    "\n",
    "Training a classifier involves estimating $P(Y|X)$.\n",
    "\n",
    "i. Generative classifiers (e.g. Naive Bayes):\n",
    "\n",
    "- Assume some functional form for $P(X,Y)$, i.e. $P(X|Y)$ and $P(Y)$\n",
    "- Estimate parameters of $P(X|Y)$, $P(Y)$ directly from training data\n",
    "- $\\underset{y}{\\arg\\max}\\quad P(X|Y)P(Y)= \\underset{y}{\\arg\\max}\\quad P(Y|X)$\n",
    "\n",
    "**Question:** _Can we learn the $P(Y|X)$ directly from data? Or better yet, can we learn decision boundary directly?_\n",
    "\n",
    "ii. Discriminative classifiers (e.g. Logistic Regression):\n",
    "\n",
    "- Assume some functional form for $P(Y|X)$ or for the decision boundary\n",
    "- Estimate parameters of $P(Y|X)$ directly from training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a classification method for binary classification problems, where input $X$ is a vector of discrete or real-valued variables and $Y$ is discrete (boolean valued). The idea is to learn $P(Y|X)$ directly from observed data.\n",
    "\n",
    "Let's consider learning $f:X\\rightarrow Y$ where,\n",
    "\n",
    "- $X$ is a vector of real-valued features, $<X_1,\\cdots,X_n>$\n",
    "- $Y$ is boolean\n",
    "- Assume all $X_i$ are conditionally independent given $Y$\n",
    "- Assume $P(X_i|Y=y_k) \\sim N(\\mu_{ik},\\sigma_i)$\n",
    "- Assume $P(Y) \\sim $ Bernoulli($\\pi$)\n",
    "\n",
    "What does this imply about the form of $P(Y|X)$?\n",
    "\n",
    "$$P(Y=0|X=<X_1,\\cdots,X_n>)=\\frac{1}{1+exp(w_0+\\sum_{i}w_iX_i)}$$\n",
    "\n",
    "\n",
    "\n",
    "$$P(Y=1|X) = 1 - P(Y=1|X) = \\frac{\\exp(w_0+\\sum_{i=1}^{n}w_iX_i)}{1+\\exp(w_0+\\sum_{i=1}^{n}w_iX_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "In Logistic regression $P(Y|X)$ follows the form of the sigmoid function, which means logistic regression gives the *probability* that an instance belongs to class $1$ or class $0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "x = np.linspace(-10,10)\n",
    "y = sigmoid(x)\n",
    "source = pd.DataFrame({\"x\":x,\"y\":y})\n",
    "base = alt.Chart(source).mark_line(size=3).encode(\n",
    "    x=\"x\",\n",
    "    y=\"y\"\n",
    ")\n",
    "a = pd.DataFrame({\"x\":[0]})\n",
    "vertline = alt.Chart(a).mark_rule(size=2).encode(\n",
    "    x=\"x\"\n",
    ")\n",
    "# base + vertline\n",
    "# base.save(\"sigmoid.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid](images/sigmoid.svg)\n",
    "*Sigmoid Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression is Linear\n",
    "\n",
    "The reason that logistic regression is linear is that, the outcome is a linear combinations of the inputs and parameters.\n",
    "\n",
    "$$\\frac{P(Y=1|X)}{P(Y=0|X)} = \\exp(w_0+\\sum_{i=1}^{n}w_iX_i)$$\n",
    "\n",
    "which implies:\n",
    "\n",
    "$$\\ln\\frac{P(Y=1|X)}{P(Y=0|X)} = w_0+\\sum_{i=1}^{n}w_iX_i$$\n",
    "\n",
    "where $w_0+\\sum_{i=1}^{n}w_iX_i$ is the linear classification rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic Regression: MLCE\n",
    "\n",
    "We have a collection of training data $D = \\{<X^1,Y^1>,\\cdots,<X^M,Y^M>\\}$. We need to find the parameters $\\mathbf{w}=<w_0,\\cdots,w_n>$ that **maximize the conditional likelihood** of the training data.\n",
    "\n",
    "Data likelihood $=\\prod_j P(<X^j,Y^j>|\\mathbf{w})$, thus data **conditional** likelihood $=\\prod_j P(Y^j|X^j,\\mathbf{w})$.\n",
    "\n",
    "therefore,\n",
    "$$W_{MLCE}=\\underset{\\mathbf{w}}{\\arg\\max}\\prod_j P(Y^j|X^j,\\mathbf{w})$$\n",
    "\n",
    "In order to make arithmetic easier, we work with the conditional log likelihood. Additionally, we know that maximizing a function is equivalent to *minimizing the negative of the function*. Therefore, we convert our problem to a minimization problem and apply the Gradient Descent algorithm to find the minimum.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  W_{MLCE}&=\\underset{\\mathbf{w}}{\\arg\\max}\\quad \\ln \\prod_j P(Y^j|X^j,\\mathbf{w}) =  \\sum_j \\ln P(Y^j|X^j,\\mathbf{w})\\\\\n",
    "  &=\\underset{W}{\\arg\\max}\\quad\\sum_j \\ln P(Y^j|X^j,\\mathbf{w})\\\\\n",
    "  &=\\underset{W}{\\arg\\min}\\quad -\\sum_j \\ln P(Y^j|X^j,\\mathbf{w})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "if $J(\\mathbf{w})=-\\sum_j \\ln P(Y^j|X^j,\\mathbf{w})$, then we have:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  J(\\mathbf{w})&=-\\sum_j \\left [ Y^j \\ln P(Y^j=1|X^j,\\mathbf{w}) + (1-Y^j) \\ln P(Y^j=0|X^j,\\mathbf{w})\\right ]\\\\\n",
    "  &=-\\sum_j \\left[ Y^j(w_0+\\sum_iw_iX_i^j)-\\ln(1+\\exp(w_0+\\sum_iw_iX_i^j))\\right ]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$J(\\mathbf{w})$ is a convex function, so we can always find global minimum. There is no closed-form solution to minimize it. However, we can use gradient descent algorithm to find the minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent algorithm for Logistic Regression\n",
    "\n",
    "1. Compute the gradient of $J_D(\\mathbf{w})$ over the entire training set $D$:\n",
    "$$\\nabla J_D(\\mathbf{w}) = \\left [\\frac{\\partial J_D(\\mathbf{w})}{\\partial w_0},\\cdots \\frac{\\partial J_D(\\mathbf{w})}{\\partial w_n} \\right]$$\n",
    "\n",
    "$$\\frac{\\partial J_D(\\mathbf{w})}{\\partial w_i}=\\sum_{j=1}^{M}X_i^j\\left[Y^j-\\hat{P}(Y^j=1|X^j,\\mathbf{w})\\right]$$\n",
    "\n",
    "We assume $X_0^j=1$, (for $j=0,1,\\cdots M$)\n",
    "\n",
    "2. Do until satisfied:\n",
    "   - Update the vector of parameters: $w_i=w_i-\\eta \\frac{\\partial J_D(\\mathbf{w})}{\\partial w_i}$, (for $i=0,1,\\cdots n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Maximum a Posteriori (MAP) to Estimate Parameters\n",
    "We assume Gaussian distributions for the prior: $\\mathbf{w} \\sim N(0,\\sigma I)$.Thus,\n",
    "\n",
    "$$\\mathbf{w^*}=\\underset{\\mathbf{w}}{\\arg\\max}\\quad \\ln\\left[P(\\mathbf{w})\\prod_j P(Y^j|X^j,\\mathbf{w})\\right]$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$J(\\mathbf{w})=-\\sum_j \\left[ Y^j \\ln P(Y^j=1|X^j,\\mathbf{w}) + (1-Y^j) \\ln P(Y^j=0|X^j,\\mathbf{w})\\right]+\\lambda\\sum_{i=1}^{n}w_i^2$$\n",
    "\n",
    "where $\\lambda \\sum_{i=1}^{n}w_i^2$ is called a **regularization** term. Regularization helps reduce the overfitting, and also keeps the weights near to zero (i.e. discourage the weights from getting large values).\n",
    "\n",
    "\n",
    "And the modified gradient descent rule is:\n",
    "\n",
    "$$w_i=w_i-\\eta \\left [\\frac{\\partial J_D(\\mathbf{w})}{\\partial w_i} + \\lambda w_i \\right ]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
